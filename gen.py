import os, sys, torch, time
import numpy as np
from typing import List
import argparse

from colorama import init, Fore, Style


init()
def print_stream(text, batch_idx):
    """æµå¼è¾“å‡ºï¼Œä½¿ç”¨listç®¡ç†æ¯ä¸ªbatchçš„è¾“å‡º"""
    colors = [
        Fore.LIGHTGREEN_EX,  # äº®ç»¿è‰²
        Fore.LIGHTBLUE_EX,   # äº®è“è‰²
        Fore.LIGHTYELLOW_EX, # äº®é»„è‰²
        Fore.LIGHTMAGENTA_EX,# äº®ç´«è‰²
        Fore.LIGHTCYAN_EX,   # äº®é’è‰²
        Fore.LIGHTRED_EX,    # äº®çº¢è‰²
        Fore.LIGHTWHITE_EX,  # äº®ç™½è‰²
        Fore.BLUE,           # è“è‰²
    ]
    
    # åˆå§‹åŒ–çŠ¶æ€å’Œbatchè¾“å‡ºåˆ—è¡¨
    if not hasattr(print_stream, 'started'):
        print_stream.started = set()
        print_stream.line_positions = {}  # è®°å½•æ¯ä¸ªbatchçš„è¡Œä½ç½®
        print_stream.total_lines = 0
        print_stream.batch_outputs = {}   # å­˜å‚¨æ¯ä¸ªbatchçš„è¾“å‡ºåˆ—è¡¨
    
    # å¦‚æœæ˜¯æ–°çš„batchï¼Œåˆå§‹åŒ–å…¶è¾“å‡ºåˆ—è¡¨å’Œä½ç½®
    if batch_idx not in print_stream.started:
        print_stream.line_positions[batch_idx] = print_stream.total_lines
        print_stream.batch_outputs[batch_idx] = []
        print(f"{colors[batch_idx % len(colors)]}[Batch {batch_idx}]: {Style.RESET_ALL}")
        print_stream.started.add(batch_idx)
        print_stream.total_lines += 1
    
    # å°†æ–°tokenæ·»åŠ åˆ°å¯¹åº”batchçš„è¾“å‡ºåˆ—è¡¨
    print_stream.batch_outputs[batch_idx].append(text)
    
    # ä¿å­˜å½“å‰å…‰æ ‡ä½ç½®
    print('\033[s', end='')
    
    # ç§»åŠ¨åˆ°å¯¹åº”batchçš„è¡Œ
    lines_up = print_stream.total_lines - print_stream.line_positions[batch_idx] - 1
    if lines_up > 0:
        print(f'\033[{lines_up}A', end='')
    
    # æ¸…é™¤å½“å‰è¡Œ
    print('\033[2K', end='')
    print('\033[0G', end='')
    
    # æ‰“å°è¯¥batchçš„å®Œæ•´è¾“å‡º
    batch_text = ''.join(print_stream.batch_outputs[batch_idx])
    print(f"{colors[batch_idx % len(colors)]}[Batch {batch_idx}]: {batch_text}{Style.RESET_ALL}", end='', flush=True)
    
    # æ¢å¤å…‰æ ‡ä½ç½®
    print('\033[u', end='')

def reset_print_stream():
    """é‡ç½®print_streamçš„çŠ¶æ€"""
    if hasattr(print_stream, 'started'):
        delattr(print_stream, 'started')
        delattr(print_stream, 'line_positions')
        delattr(print_stream, 'total_lines')

    
class StreamWriter:

    """å¤šæ–‡ä»¶æµå¼è¾“å‡º"""
    def __init__(self, num_batches, output_dir="outputs"):
        os.makedirs(output_dir, exist_ok=True)
        self.files = [
            open(f"{output_dir}/output_{i}.txt", "w", encoding="utf-8") 
            for i in range(num_batches)
        ]
    
    def callback(self, text, batch_idx):
        self.files[batch_idx].write(text)
        self.files[batch_idx].flush()
    
    def close(self):
        for f in self.files:
            f.close()
def batch_inference(ctx_list: List[str], args, callback=None):
    """
    æ‰¹é‡æ¨ç†å‡½æ•°
    
    Args:
        ctx_list: è¾“å…¥ä¸Šä¸‹æ–‡åˆ—è¡¨
        args: è§£æåçš„å‘½ä»¤è¡Œå‚æ•°
    
    Returns:
        tuple: (answer, state) ç”Ÿæˆçš„ç­”æ¡ˆå’ŒçŠ¶æ€
    """
    # è®¾ç½®ç¯å¢ƒå˜é‡
    os.environ['RWKV_JIT_ON'] = args.jit_on
    os.environ["RWKV_CUDA_ON"] = args.cuda_on
    os.environ["RWKV_fla_ON"] = args.fla_on
    
    # å¯¼å…¥å¿…è¦çš„æ¨¡å—
    from rwkv.model import RWKV
    from rwkv.utils import PIPELINE, PIPELINE_ARGS
    
    # åˆå§‹åŒ–æ¨¡å‹å’Œpipeline
    model = RWKV(model=args.base_model, strategy='cuda fp16')
    pipeline = PIPELINE(model, "rwkv_vocab_v20230424")
    
    # åŠ è½½çŠ¶æ€
    states = torch.load(args.state_file)
    states_value = []
    
    # åˆå§‹åŒ–çŠ¶æ€
    for i in range(model.args.n_layer):
        key = f'blocks.{i}.att.time_state'
        value = states[key]
        
        prev_x = torch.zeros(args.batch_size, model.args.n_embd, 
                           device=args.device, dtype=torch.float16)
        prev_states = value.clone().detach().to(
            device=args.device, 
            dtype=torch.float32
        ).transpose(1, 2).expand(args.batch_size, *value.shape)
        prev_ffn = torch.zeros(args.batch_size, model.args.n_embd, 
                             device=args.device, dtype=torch.float16)
        
        states_value.append(prev_x)
        states_value.append(prev_states)
        states_value.append(prev_ffn)
    
    # è®¾ç½®ç”Ÿæˆå‚æ•°
    pipeline_args = PIPELINE_ARGS(
        temperature=args.temperature,
        top_p=args.top_p,
        top_k=args.top_k,
        alpha_frequency=args.alpha_frequency,
        alpha_presence=args.alpha_presence,
        alpha_decay=args.alpha_decay,
        token_ban=args.token_ban,
        token_stop=args.token_stop,
        chunk_len=args.chunk_len
    )
    
    # æ‰§è¡Œæ‰¹é‡æ¨ç†
  
    try:
        reset_print_stream()
        answer, state = pipeline.gen_bsz(
            ctx_list, 
            state=states_value, 
            token_count=args.token_count, 
            args=pipeline_args,
            callback=callback
        )
    finally:
        if hasattr(print_stream, 'total_lines'):
            # ç¡®ä¿æœ€åå…‰æ ‡åœ¨æ‰€æœ‰è¾“å‡ºçš„ä¸‹æ–¹
            print('\n' * (print_stream.total_lines - 1))
        pipeline.reset_cache()
    
    
    return answer, state

if __name__ == "__main__":
    def parse_args():
        parser = argparse.ArgumentParser(description='RWKV Batch Inference')
        
        # ç¯å¢ƒè®¾ç½®
        parser.add_argument('--jit_on', type=str, default='1', help='JIT compilation switch')
        parser.add_argument('--cuda_on', type=str, default='1', help='CUDA switch')
        parser.add_argument('--fla_on', type=str, default='0', help='FLA switch')
        parser.add_argument('--device', type=str, default='cuda', help='Device to use')
        
        # æ¨¡å‹å’ŒçŠ¶æ€è®¾ç½®
        parser.add_argument('--base_model', type=str, 
                           default='/home/rwkv/Peter/model/base/v3-7b-industry-instruct-1024.pth',
                           help='Path to base model')
        parser.add_argument('--state_file', type=str,
                           default='/home/rwkv/Peter/model/state/entityet/v3-7b-instruct-entitieset-1024-state.pth',
                           help='Path to state file')
        
        # æ‰¹å¤„ç†è®¾ç½®
        parser.add_argument('--batch_size', type=int, default=2, help='Batch size')
        parser.add_argument('--token_count', type=int, default=500, help='Number of tokens to generate')
        
        # é‡‡æ ·å‚æ•°
        parser.add_argument('--temperature', type=float, default=1.0, help='Sampling temperature')
        parser.add_argument('--top_p', type=float, default=0, help='Top-p sampling threshold')
        parser.add_argument('--top_k', type=int, default=0, help='Top-k sampling threshold')
        parser.add_argument('--alpha_frequency', type=float, default=0, help='Frequency penalty')
        parser.add_argument('--alpha_presence', type=float, default=0, help='Presence penalty')
        parser.add_argument('--alpha_decay', type=float, default=0, help='Decay factor')
        parser.add_argument('--chunk_len', type=int, default=256, help='Chunk length for processing')
        
        # ç‰¹æ®Štokenè®¾ç½®
        parser.add_argument('--token_ban', type=int, nargs='+', default=[0], help='Banned tokens')
        parser.add_argument('--token_stop', type=int, nargs='+', default=[24], help='Stop tokens')
        parser.add_argument('--vocab_size', type=int, default=65536, help='Vocabulary size')

        parser.add_argument('--output_dir', type=str, default='outputs', help='Output directory')
        parser.add_argument('--output_mode', type=str, choices=['console', 'file'], 
                        default='console', help='Output mode')
        
        return parser.parse_args()

    def main():
        # è§£æå‘½ä»¤è¡Œå‚æ•°
        args = parse_args()
        
        instruction = 'è¯·ä»è¾“å…¥æ–‡æœ¬ä¸­è¯†åˆ«å®ä½“ï¼Œå¯¹æ¯ä¸ªå®ä½“æå–ï¼š1)å®ä½“åç§° 2)å®ä½“ç±»å‹(ä»ç»™å®šç±»å‹åˆ—è¡¨ä¸­é€‰æ‹©) 3)å®ä½“çš„è¯¦ç»†æè¿°,åŒ…æ‹¬å±æ€§å’Œæ´»åŠ¨ã€‚æŒ‰æ ¼å¼è¾“å‡ºï¼š(entity,å®ä½“åç§°,å®ä½“ç±»å‹,æè¿°)ğŸ˜º.'
        # å‡†å¤‡è¾“å…¥æ•°æ®
        input_text1 = 'æœ¬ä¹¦ä¸»è§’ï¼Œç©¿è¶Šè€…ï¼Œå¸¦ç€æˆå¹´äººçš„è®°å¿†ä»åœ°çƒè½¬ç”Ÿåˆ°æ–—æ°”å¤§é™†ã€‚ [2]æœ¬ä¸ºå¤©æ‰å°‘å¹´ï¼Œä½†ä»åä¸€å²é‚£å¹´å¼€å§‹è¿ç»­ä¸‰å¹´å¤šè«åå…¶å¦™åœ°é€€åŒ–æˆæ–—ä¹‹æ°”ä¸‰æ®µï¼Œä»æ­¤é€æ¸æ²¦ä¸ºé­äººç™½çœ¼çš„åºŸæŸ´ã€‚ä¹‹åå¾—çŸ¥åŸå› ç«Ÿæ˜¯æœ‰ä¸€ä¸ªç¥ç§˜çš„çµé­‚"è¯è€"è—åœ¨è§ç‚æ¯äº²çš„é—ç‰©æˆ’æŒ‡ä¸­ä¸æ–­å¸æ”¶ä»–çš„æ–—ä¹‹æ°”ï¼Œåœ¨è¯è€åœæ­¢å¸æ”¶æ–—ä¹‹æ°”å¹¶ç­”åº”å¸®ä»–é‡å±•å¤©èµ„åï¼Œä¸€å¹´æ—¶é—´å†…çªç ´è‡³æ–—ä¹‹æ°”ä¹æ®µï¼Œéœ‡æƒŠå…¨åŸ'
        input_text2 ='ç”·ä¸»è§’è§ç‚çš„è€å¸ˆã€‚äººç§°è¯è€ã€è¯å°Šè€…ï¼Œåç§°è¯åœ£ï¼Œæ‹¥æœ‰"éª¨çµå†·ç«"ï¼ˆåèµ ä¸è§ç‚ï¼‰ã€‚æ˜Ÿé™¨é˜æå°‘éœ²é¢çš„é˜ä¸»ï¼ˆå‰¯é˜ä¸»ä¸ºè‡³äº¤å¥½å‹é£å°Šè€…-é£é—²ï¼‰ï¼Œåˆä¸ºä¹è½¬æ–—å°Šå·…å³°å¼ºè€…ï¼Œä¹å“ç‚¼è¯å®—å¸ˆã€‚å› é­å›å¾’éŸ©æ«å‡ºå–è€Œè½éš¾æˆä¸ºçµé­‚çŠ¶æ€ï¼Œæ½œä¼äºä¸€ä¸ªæˆ’æŒ‡ä¸­ï¼Œåæˆ’æŒ‡è¾—è½¬è½å…¥è§ç‚ä¹‹æ‰‹ï¼Œåœ¨å¸æ”¶è§ç‚ä¸‰å¹´æ–—ä¹‹æ°”åæ¢å¤æ„è¯†ã€‚'  # ä½ çš„è¾“å…¥æ–‡æœ¬
        entities_types = 'è§’è‰²,å¯¼å¸ˆ,ä¿®ç‚¼ä½“ç³»,ç›®æ ‡,æ€§æ ¼ç‰¹ç‚¹'
        
        ctx1 = f'Instruction: {instruction}\n\nInput: æ–‡æœ¬: {input_text1}\nå®ä½“ç±»å‹: {entities_types}\n\nResponse:'
        ctx2 = f'Instruction: {instruction}\n\nInput: æ–‡æœ¬: {input_text2}\nå®ä½“ç±»å‹: {entities_types}\n\nResponse:'
        
        print(len(instruction), len(input_text1), len(input_text2))
        ctx_list = [ctx1,ctx2]
        
        
        # æ‰§è¡Œæ‰¹é‡æ¨ç†
        # è®¾ç½®è¾“å‡ºå›è°ƒ
        if args.output_mode == 'file':
            writer = StreamWriter(len(ctx_list), args.output_dir)
            callback = writer.callback
        else:
            callback = print_stream
        
        try:
            # æ‰§è¡Œæ‰¹é‡æ¨ç†
            answer, state = batch_inference(ctx_list, args, callback=callback)
            
            # æ‰“å°æœ€ç»ˆç»“æœ
            print(f"\n{Fore.CYAN}Final outputs:{Style.RESET_ALL}")
            for i, ans in answer.items():
                print(f"{Fore.GREEN}Batch {i}:{Style.RESET_ALL}\n{ans}\n")
                
        finally:
            # å¦‚æœä½¿ç”¨æ–‡ä»¶è¾“å‡ºï¼Œç¡®ä¿å…³é—­æ–‡ä»¶
            if args.output_mode == 'file':
                writer.close()

    main()

